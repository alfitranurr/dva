{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-Encoding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from .resnet import Res12_Quadratic\n",
    "from .diffusion_process import GaussianDiffusion, get_beta_schedule\n",
    "from .encoder import Encoder\n",
    "from .embedding import DataEmbedding\n",
    "\n",
    "\n",
    "class diffusion_generate(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.target_dim = args.target_dim\n",
    "        self.input_size = args.embedding_dimension\n",
    "        self.prediction_length = args.prediction_length\n",
    "        self.seq_length = args.sequence_length\n",
    "        self.scale = args.scale\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=args.hidden_size,\n",
    "            num_layers=args.num_layers,\n",
    "            dropout=args.dropout_rate,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.generative = Encoder(args)\n",
    "        self.diffusion = GaussianDiffusion(\n",
    "            self.generative,\n",
    "            input_size=args.target_dim,\n",
    "            diff_steps=args.diff_steps,\n",
    "            beta_end=args.beta_end,\n",
    "            beta_schedule=args.beta_schedule,\n",
    "            scale = args.scale,\n",
    "        )\n",
    "        self.projection = nn.Linear(args.embedding_dimension+args.hidden_size, args.embedding_dimension)\n",
    "\n",
    "    def forward(self, past_time_feat, future_time_feat, t):\n",
    "        time_feat, _ = self.rnn(past_time_feat)\n",
    "        input = torch.cat([time_feat, past_time_feat], dim=-1)\n",
    "        output, y_noisy = self.diffusion.log_prob(input, future_time_feat, t)\n",
    "        return output, y_noisy\n",
    "\n",
    "\n",
    "class denoise_net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "\n",
    "        # ResNet that used to calculate the scores.\n",
    "        self.score_net = Res12_Quadratic(1, 64, 32, normalize=False, AF=nn.ELU())\n",
    "\n",
    "        # Generate the diffusion schedule.\n",
    "        sigmas = get_beta_schedule(args.beta_schedule, args.beta_start, args.beta_end, args.diff_steps)\n",
    "        alphas = 1.0 - sigmas*0.5\n",
    "        self.alphas_cumprod = torch.tensor(np.cumprod(alphas, axis=0))\n",
    "        self.sqrt_alphas_cumprod = torch.tensor(np.sqrt(np.cumprod(alphas, axis=0)))\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.tensor(np.sqrt(1-np.cumprod(alphas, axis=0)))\n",
    "        self.sigmas = torch.tensor(1. - self.alphas_cumprod)\n",
    "\n",
    "        # The generative bvae model.\n",
    "        self.diffusion_gen = diffusion_generate(args)\n",
    "\n",
    "        # Data embedding module.\n",
    "        self.embedding = DataEmbedding(args.input_dim, args.embedding_dimension, args.dropout_rate)\n",
    "\n",
    "    def extract(self, a, t, x_shape):\n",
    "        b, *_ = t.shape\n",
    "        out = a.gather(-1, t)\n",
    "        return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "    def forward(self, past_time_feat, mark, future_time_feat, t):\n",
    "        # Embed the original time series.\n",
    "        input = self.embedding(past_time_feat, mark)\n",
    "\n",
    "        # Output the distribution of the generative results, the sampled generative results and the total correlations of the generative model.\n",
    "        output, y_noisy = self.diffusion_gen(input, future_time_feat, t)\n",
    "\n",
    "        # Score matching.\n",
    "        sigmas_t = self.extract(self.sigmas.to(y_noisy.device), t, y_noisy.shape)\n",
    "        y = future_time_feat.unsqueeze(1).float()\n",
    "        y_noisy1 = output.sample().float().requires_grad_()\n",
    "        E = self.score_net(y_noisy1).sum()\n",
    "\n",
    "        # The Loss of multiscale score matching.\n",
    "        grad_x = torch.autograd.grad(E, y_noisy1, create_graph=True)[0]\n",
    "        dsm_loss = torch.mean(torch.sum(((y-y_noisy1.detach())+grad_x*1)**2*sigmas_t, [1,2,3])).float()\n",
    "        return output, y_noisy, dsm_loss\n",
    "\n",
    "\n",
    "class pred_net(denoise_net):\n",
    "    def forward(self, x, mark):\n",
    "        input = self.embedding(x, mark)\n",
    "        x_t, _ = self.diffusion_gen.rnn(input)\n",
    "        input = torch.cat([x_t, input], dim=-1)\n",
    "        input = input.unsqueeze(1)\n",
    "        logits = self.diffusion_gen.generative(input)\n",
    "        output = self.diffusion_gen.generative.decoder_output(logits)\n",
    "        y = output.mu.float().requires_grad_()\n",
    "\n",
    "        E = self.score_net(y).sum()\n",
    "        grad_x = torch.autograd.grad(E, y, create_graph=True)[0]\n",
    "        out = y - grad_x*1\n",
    "        return y, out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, neg_slope=0.2, latent_dim=10, hidden_units=1000, out_units=2):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Activation parameters\n",
    "        self.neg_slope = neg_slope\n",
    "        self.leaky_relu = nn.LeakyReLU(self.neg_slope, True)\n",
    "\n",
    "        # Layer parameters\n",
    "        self.z_dim = latent_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        # theoretically 1 with sigmoid but gives bad results => use 2 and softmax\n",
    "        out_units = out_units\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.lin1 = nn.Linear(self.z_dim, hidden_units)\n",
    "        self.lin2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.lin3 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.lin4 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.lin5 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.lin6 = nn.Linear(hidden_units, out_units)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Fully connected layers with leaky ReLu activations\n",
    "        z = self.leaky_relu(self.lin1(z))\n",
    "        z = self.leaky_relu(self.lin2(z))\n",
    "        z = self.leaky_relu(self.lin3(z))\n",
    "        z = self.leaky_relu(self.lin4(z))\n",
    "        z = self.leaky_relu(self.lin5(z))\n",
    "        z = self.lin6(z)\n",
    "        return z\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
