{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d493e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-Encoding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "BN_EPS = 1e-5\n",
    "SYNC_BN = False\n",
    "\n",
    "OPS = OrderedDict([\n",
    "    ('res_elu', lambda Cin, Cout, stride: ELUConv(Cin, Cout, 3, stride, 1)),\n",
    "    ('res_bnelu', lambda Cin, Cout, stride: BNELUConv(Cin, Cout, 3, stride, 1)),\n",
    "    ('res_bnswish', lambda Cin, Cout, stride: BNSwishConv(Cin, Cout, 3, stride, 1)),\n",
    "    ('res_bnswish5', lambda Cin, Cout, stride: BNSwishConv(Cin, Cout, 3, stride, 2, 2)),\n",
    "    ('mconv_e6k5g0', lambda Cin, Cout, stride: InvertedResidual(Cin, Cout, stride, ex=6, dil=1, k=5, g=1)),\n",
    "    ('mconv_e3k5g0', lambda Cin, Cout, stride: InvertedResidual(Cin, Cout, stride, ex=3, dil=1, k=5, g=1)),\n",
    "    ('mconv_e3k5g8', lambda Cin, Cout, stride: InvertedResidual(Cin, Cout, stride, ex=3, dil=1, k=5, g=8)),\n",
    "    ('mconv_e6k11g0', lambda Cin, Cout, stride: InvertedResidual(Cin, Cout, stride, ex=6, dil=1, k=11, g=0)),\n",
    "])\n",
    "\n",
    "\n",
    "class SyncBatchNormSwish(_BatchNorm):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n",
    "                 track_running_stats=True, process_group=None):\n",
    "        super(SyncBatchNormSwish, self).__init__(num_features, eps, momentum, affine, track_running_stats)\n",
    "        self.process_group = process_group\n",
    "        self.ddp_gpu_size = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        exponential_average_factor = self.momentum\n",
    "        out = F.batch_norm(\n",
    "            input, self.running_mean, self.running_var, self.weight, self.bias,\n",
    "            self.training or not self.track_running_stats,\n",
    "            exponential_average_factor, self.eps)\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_skip_connection(C, stride, channel_mult):\n",
    "    if stride == 1:\n",
    "        return Identity()\n",
    "    elif stride == 2:\n",
    "        return FactorizedReduce(C, int(channel_mult * C))\n",
    "    elif stride == -1:\n",
    "        return nn.Sequential(UpSample(), Conv2D(C, int(C / channel_mult), kernel_size=1))\n",
    "\n",
    "\n",
    "def norm(t, dim):\n",
    "    return torch.sqrt(torch.sum(t * t, dim))\n",
    "\n",
    "\n",
    "def logit(t):\n",
    "    return torch.log(t) - torch.log(1 - t)\n",
    "\n",
    "\n",
    "def act(t):\n",
    "    # The following implementation has lower memory.\n",
    "    return SwishFN.apply(t)\n",
    "\n",
    "\n",
    "class SwishFN(torch.autograd.Function):\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = torch.sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return act(x)\n",
    "\n",
    "\n",
    "def normalize_weight_jit(log_weight_norm, weight):\n",
    "    n = torch.exp(log_weight_norm)\n",
    "    wn = torch.sqrt(torch.sum(weight * weight, dim=[1, 2, 3]))   # norm(w)\n",
    "    weight = n * weight / (wn.view(-1, 1, 1, 1) + 1e-5)\n",
    "    return weight\n",
    "\n",
    "\n",
    "class Conv2D(nn.Conv2d):\n",
    "    \"\"\"Allows for weights as input.\"\"\"\n",
    "\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False, data_init=False,\n",
    "                 weight_norm=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            use_shared (bool): Use weights for this layer or not?\n",
    "        \"\"\"\n",
    "        super(Conv2D, self).__init__(C_in, C_out, kernel_size, stride, padding, dilation, groups, bias)\n",
    "\n",
    "        self.log_weight_norm = None\n",
    "        if weight_norm:\n",
    "            init = norm(self.weight, dim=[1, 2, 3]).view(-1, 1, 1, 1)\n",
    "            self.log_weight_norm = nn.Parameter(torch.log(init + 1e-2), requires_grad=True)\n",
    "\n",
    "        self.data_init = data_init\n",
    "        self.init_done = False\n",
    "        self.weight_normalized = self.normalize_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # do data based initialization\n",
    "        self.weight_normalized = self.normalize_weight()\n",
    "        #print(self.weight_normalized.shape)\n",
    "        bias = self.bias\n",
    "        return F.conv2d(x, self.weight_normalized, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def normalize_weight(self):\n",
    "        \"\"\" applies weight normalization \"\"\"\n",
    "        if self.log_weight_norm is not None:\n",
    "            weight = normalize_weight_jit(self.log_weight_norm, self.weight)\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        return weight\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class SyncBatchNorm(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(SyncBatchNorm, self).__init__()\n",
    "        self.bn = nn.BatchNorm(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(x)\n",
    "\n",
    "\n",
    "# quick switch between multi-gpu, single-gpu batch norm\n",
    "def get_batchnorm(*args, **kwargs):\n",
    "    return nn.BatchNorm2d(*args, **kwargs)\n",
    "\n",
    "\n",
    "class ELUConv(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(ELUConv, self).__init__()\n",
    "        self.upsample = stride == -1\n",
    "        stride = abs(stride)\n",
    "        self.conv_0 = Conv2D(C_in, C_out, kernel_size, stride=stride, padding=padding, bias=True, dilation=dilation,\n",
    "                             data_init=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.elu(x)\n",
    "        if self.upsample:\n",
    "            out = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.conv_0(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BNELUConv(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BNELUConv, self).__init__()\n",
    "        self.upsample = stride == -1\n",
    "        stride = abs(stride)\n",
    "        self.bn = get_batchnorm(C_in, eps=BN_EPS, momentum=0.05)\n",
    "        self.conv_0 = Conv2D(C_in, C_out, kernel_size, stride=stride, padding=padding, bias=True, dilation=dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        out = F.elu(x)\n",
    "        if self.upsample:\n",
    "            out = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.conv_0(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BNSwishConv(nn.Module):\n",
    "    \"\"\"ReLU + Conv2d + BN.\"\"\"\n",
    "\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BNSwishConv, self).__init__()\n",
    "        self.upsample = stride == -1\n",
    "        stride = abs(stride)\n",
    "        self.bn_act = SyncBatchNormSwish(C_in, eps=BN_EPS, momentum=0.05)\n",
    "        self.conv_0 = Conv2D(C_in, C_out, kernel_size, stride=stride, padding=padding, bias=True, dilation=dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): of size (B, C_in, H, W)\n",
    "        \"\"\"\n",
    "        out = self.bn_act(x)\n",
    "        if self.upsample:\n",
    "            out = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.conv_0(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FactorizedReduce(nn.Module):\n",
    "    def __init__(self, C_in, C_out):\n",
    "        super(FactorizedReduce, self).__init__()\n",
    "        assert C_out % 2 == 0\n",
    "        self.conv_1 = Conv2D(C_in, C_out // 4, 1, stride=2, padding=0, bias=True)\n",
    "        self.conv_2 = Conv2D(C_in, C_out // 4, 1, stride=2, padding=0, bias=True)\n",
    "        self.conv_3 = Conv2D(C_in, C_out // 4, 1, stride=2, padding=0, bias=True)\n",
    "        self.conv_4 = Conv2D(C_in, C_out - 3 * (C_out // 4), 1, stride=2, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = act(x)\n",
    "        conv1 = self.conv_1(out[:,:,:, :])\n",
    "        conv2 = self.conv_2(out[:, :, 1:, :])\n",
    "        conv3 = self.conv_3(out[:, :, :, :])\n",
    "        conv4 = self.conv_4(out[:, :, 1:, :])\n",
    "        out = torch.cat([conv1, conv2, conv3, conv4], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UpSample, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "\n",
    "class EncCombinerCell(nn.Module):\n",
    "    def __init__(self, Cin1, Cin2, Cout, cell_type):\n",
    "        super(EncCombinerCell, self).__init__()\n",
    "        self.cell_type = cell_type\n",
    "        # Cin = Cin1 + Cin2\n",
    "        self.conv = Conv2D(Cin2, Cout, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x2 = self.conv(x2)\n",
    "        out = x1 + x2\n",
    "        return out\n",
    "\n",
    "\n",
    "# original combiner\n",
    "class DecCombinerCell(nn.Module):\n",
    "    def __init__(self, Cin1, Cin2, Cout, cell_type):\n",
    "        super(DecCombinerCell, self).__init__()\n",
    "        self.cell_type = cell_type\n",
    "        self.conv = Conv2D(Cin1 + Cin2, Cout, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out = torch.cat([x1, x2], dim=1)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvBNSwish(nn.Module):\n",
    "    def __init__(self, Cin, Cout, k=3, stride=1, groups=1, dilation=1):\n",
    "        padding = dilation * (k - 1) // 2\n",
    "        super(ConvBNSwish, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2D(Cin, Cout, k, stride, padding, groups=groups, bias=False, dilation=dilation, weight_norm=False),\n",
    "            SyncBatchNormSwish(Cout, eps=BN_EPS, momentum=0.05)  # drop in replacement for BN + Swish\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, Cin, Cout):\n",
    "        super(SE, self).__init__()\n",
    "        num_hidden = max(Cout // 16, 4)\n",
    "        self.se = nn.Sequential(nn.Linear(Cin, num_hidden), nn.ReLU(inplace=True),\n",
    "                                nn.Linear(num_hidden, Cout), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        se = torch.mean(x, dim=[2, 3])\n",
    "        se = se.view(se.size(0), -1)\n",
    "        se = self.se(se)\n",
    "        se = se.view(se.size(0), -1, 1, 1)\n",
    "        return x * se\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, Cin, Cout, stride, ex, dil, k, g):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2, -1]\n",
    "\n",
    "        hidden_dim = int(round(Cin * ex))\n",
    "        self.use_res_connect = self.stride == 1 and Cin == Cout\n",
    "        self.upsample = self.stride == -1\n",
    "        self.stride = abs(self.stride)\n",
    "        groups = hidden_dim if g == 0 else g\n",
    "\n",
    "        layers0 = [nn.UpsamplingNearest2d(scale_factor=2)] if self.upsample else []\n",
    "        layers = [get_batchnorm(Cin, eps=BN_EPS, momentum=0.05),\n",
    "                  ConvBNSwish(Cin, hidden_dim, k=1),\n",
    "                  ConvBNSwish(hidden_dim, hidden_dim, stride=self.stride, groups=groups, k=k, dilation=dil),\n",
    "                  Conv2D(hidden_dim, Cout, 1, 1, 0, bias=False, weight_norm=False),\n",
    "                  get_batchnorm(Cout, momentum=0.05)]\n",
    "\n",
    "        layers0.extend(layers)\n",
    "        self.conv = nn.Sequential(*layers0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
