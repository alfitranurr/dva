{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fedfd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-Encoding: utf-8 -*-\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from .neural_operations import OPS, EncCombinerCell, DecCombinerCell, Conv2D, get_skip_connection\n",
    "from .utils import get_stride_for_cell_type, get_input_size, groups_per_scale, get_arch_cells\n",
    "\n",
    "\n",
    "class Cell(nn.Module):\n",
    "    def __init__(self, Cin, Cout, cell_type, arch, use_se):\n",
    "        super(Cell, self).__init__()\n",
    "        self.cell_type = cell_type\n",
    "        stride = get_stride_for_cell_type(self.cell_type)\n",
    "        self.skip = get_skip_connection(Cin, stride, channel_mult=2)\n",
    "        self.use_se = use_se\n",
    "        self._num_nodes = len(arch)\n",
    "        self._ops = nn.ModuleList()\n",
    "        for i in range(self._num_nodes):\n",
    "            stride = get_stride_for_cell_type(self.cell_type) if i == 0 else 1\n",
    "            if i==0:\n",
    "                primitive = arch[i]\n",
    "                op = OPS[primitive](Cin, Cout, stride)\n",
    "            else:\n",
    "                primitive = arch[i]\n",
    "                op = OPS[primitive](Cout, Cout, stride)\n",
    "            self._ops.append(op)\n",
    "\n",
    "    def forward(self, s):\n",
    "        # skip branch\n",
    "        skip = self.skip(s)\n",
    "        for i in range(self._num_nodes):\n",
    "            s = self._ops[i](s)\n",
    "        return skip + 0.1 * s\n",
    "\n",
    "\n",
    "def soft_clamp5(x: torch.Tensor):\n",
    "    return x.div(5.).tanh_().mul(5.)\n",
    "\n",
    "\n",
    "def sample_normal_jit(mu, sigma):\n",
    "    eps = mu.mul(0).normal_()\n",
    "    # print(eps)\n",
    "    z = eps.mul_(sigma).add_(mu)\n",
    "    # print(z.shape)\n",
    "    return z, eps\n",
    "\n",
    "\n",
    "class Normal:\n",
    "    def __init__(self, mu, log_sigma, temp=1.):\n",
    "        self.mu = soft_clamp5(mu)\n",
    "        log_sigma = soft_clamp5(log_sigma)\n",
    "        self.sigma = torch.exp(log_sigma)\n",
    "        if temp != 1.:\n",
    "            self.sigma *= temp\n",
    "\n",
    "    def sample(self):\n",
    "        return sample_normal_jit(self.mu, self.sigma)\n",
    "\n",
    "    def sample_given_eps(self, eps):\n",
    "        return eps * self.sigma + self.mu\n",
    "\n",
    "    def log_p(self, samples):\n",
    "        normalized_samples = (samples - self.mu) / self.sigma\n",
    "        log_p = - 0.5 * normalized_samples * normalized_samples - 0.5 * np.log(2 * np.pi) - torch.log(self.sigma)\n",
    "        return log_p\n",
    "\n",
    "    def kl(self, normal_dist):\n",
    "        term1 = (self.mu - normal_dist.mu) / normal_dist.sigma\n",
    "        term2 = self.sigma / normal_dist.sigma\n",
    "        return 0.5 * (term1 * term1 + term2 * term2) - 0.5 - torch.log(term2)\n",
    "\n",
    "\n",
    "class NormalDecoder:\n",
    "    def __init__(self, param):\n",
    "        B, C, H, W = param.size()\n",
    "        self.num_c = C // 2\n",
    "        self.mu = param[:, :self.num_c, :, :]                                 # B, 3, H, W\n",
    "        self.log_sigma = param[:, self.num_c:, :, :]                          # B, 3, H, W\n",
    "        self.sigma = torch.exp(self.log_sigma) + 1e-2\n",
    "        self.dist = Normal(self.mu, self.log_sigma)\n",
    "\n",
    "    def log_prob(self, samples):\n",
    "        return self.dist.log_p(samples)\n",
    "\n",
    "    def sample(self,):\n",
    "        x, _ = self.dist.sample()\n",
    "        return x\n",
    "\n",
    "\n",
    "def log_density_gaussian(sample, mu, logvar):\n",
    "    normalization = - 0.5 * (math.log(2 * math.pi) + logvar)\n",
    "    inv_var = torch.exp(-logvar)\n",
    "    log_density = normalization - 0.5 * ((sample - mu)**2 * inv_var)\n",
    "    log_qz = torch.logsumexp(torch.sum(log_density, [2,3]), dim=1, keepdim=False)\n",
    "    log_prod_qzi = torch.logsumexp(log_density, dim=1, keepdim=False).sum((1,2))\n",
    "    loss_p_z = (log_qz - log_prod_qzi)\n",
    "    loss_p_z = ((loss_p_z - torch.min(loss_p_z))/(torch.max(loss_p_z)-torch.min(loss_p_z))).mean()\n",
    "    return loss_p_z\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.channel_mult = args.channel_mult\n",
    "        self.mult = args.mult\n",
    "        self.prediction_length = args.prediction_length\n",
    "        self.num_preprocess_blocks = args.num_preprocess_blocks\n",
    "        self.num_preprocess_cells = args.num_preprocess_cells\n",
    "        self.num_channels_enc = args.num_channels_enc\n",
    "        self.arch_instance = get_arch_cells(args.arch_instance)\n",
    "        self.stem = Conv2D(1, args.num_channels_enc, 3, padding=1, bias=True)\n",
    "        self.num_latent_per_group = args.num_latent_per_group\n",
    "\n",
    "        self.num_channels_dec = args.num_channels_dec\n",
    "        self.groups_per_scale = args.groups_per_scale\n",
    "        self.num_postprocess_blocks = args.num_postprocess_blocks\n",
    "        self.num_postprocess_cells = args.num_postprocess_cells\n",
    "        self.use_se = False\n",
    "        self.input_size = args.embedding_dimension\n",
    "        self.hidden_size = args.hidden_size\n",
    "        self.projection = nn.Linear(args.embedding_dimension+args.hidden_size, args.target_dim)\n",
    "\n",
    "        c_scaling = self.channel_mult ** (self.num_preprocess_blocks) #4\n",
    "        spatial_scaling = 2 ** (self.num_preprocess_blocks) #4\n",
    "\n",
    "        prior_ftr0_size = (int(c_scaling * self.num_channels_dec), args.prediction_length// spatial_scaling,\n",
    "                           (args.embedding_dimension + args.hidden_size + 1) // spatial_scaling)\n",
    "        self.prior_ftr0 = nn.Parameter(torch.rand(size=prior_ftr0_size), requires_grad=True)\n",
    "        self.z0_size = [self.num_latent_per_group, args.prediction_length // spatial_scaling, (args.embedding_dimension+ args.hidden_size + 1) // spatial_scaling]\n",
    "\n",
    "        self.pre_process = self.init_pre_process(args.mult)\n",
    "        self.enc_tower = self.init_encoder_tower(self.mult)\n",
    "\n",
    "        self.enc0 = nn.Sequential(nn.ELU(), Conv2D(self.num_channels_enc * self.mult,\n",
    "                        self.num_channels_enc * self.mult, kernel_size=1, bias=True), nn.ELU())\n",
    "\n",
    "        self.enc_sampler, self.dec_sampler = self.init_sampler(self.mult)\n",
    "\n",
    "        self.dec_tower = self.init_decoder_tower(self.mult)\n",
    "\n",
    "        self.post_process = self.init_post_process(self.mult)\n",
    "        self.image_conditional = nn.Sequential(nn.ELU(),\n",
    "                             Conv2D(int(self.num_channels_dec * self.mult), 2, 3, padding=1, bias=True))\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=args.sequence_length,\n",
    "            hidden_size=args.prediction_length,\n",
    "            num_layers=args.num_layers,\n",
    "            dropout=args.dropout_rate,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def init_pre_process(self, mult):\n",
    "        pre_process = nn.ModuleList()\n",
    "        for b in range(self.num_preprocess_blocks):\n",
    "            for c in range(self.num_preprocess_cells):\n",
    "                if c == self.num_preprocess_cells - 1:\n",
    "                    arch = self.arch_instance['down_pre']\n",
    "                    num_ci = int(self.num_channels_enc * mult)\n",
    "                    num_co = int(self.channel_mult * num_ci)\n",
    "                    cell = Cell(num_ci, num_co, cell_type='down_pre', arch=arch, use_se=self.use_se)\n",
    "                    mult = self.channel_mult * mult\n",
    "                else:\n",
    "                    arch = self.arch_instance['normal_pre']\n",
    "                    num_c = self.num_channels_enc * mult\n",
    "                    cell = Cell(num_c, num_c, cell_type='normal_pre', arch=arch, use_se=self.use_se)\n",
    "                pre_process.append(cell)\n",
    "        self.mult = mult\n",
    "        return pre_process\n",
    "\n",
    "    def init_encoder_tower(self, mult):\n",
    "        enc_tower = nn.ModuleList()\n",
    "        for g in range(self.groups_per_scale):\n",
    "            arch = self.arch_instance['normal_enc']\n",
    "            num_c = int(self.num_channels_enc * mult)\n",
    "            cell = Cell(num_c, num_c, cell_type='normal_enc', arch=arch, use_se=self.use_se)\n",
    "            enc_tower.append(cell)\n",
    "\n",
    "            if not (g == self.groups_per_scale - 1):\n",
    "                num_ce = int(self.num_channels_enc * mult)\n",
    "                num_cd = int(self.num_channels_dec * mult)\n",
    "                cell = EncCombinerCell(num_ce, num_cd, num_ce, cell_type='combiner_enc')\n",
    "                enc_tower.append(cell)\n",
    "\n",
    "        self.mult = mult\n",
    "        return enc_tower\n",
    "\n",
    "    def init_decoder_tower(self, mult):\n",
    "\n",
    "        dec_tower = nn.ModuleList()\n",
    "        for g in range(self.groups_per_scale):\n",
    "            num_c = int(self.num_channels_dec * mult)\n",
    "            if not (g == 0):\n",
    "                arch = self.arch_instance['normal_dec']\n",
    "                cell = Cell(num_c, num_c, cell_type='normal_dec', arch=arch, use_se=self.use_se)\n",
    "                dec_tower.append(cell)\n",
    "            #print(num_c)\n",
    "            cell = DecCombinerCell(num_c, self.num_latent_per_group, num_c, cell_type='combiner_dec')\n",
    "            dec_tower.append(cell)\n",
    "        self.mult = mult\n",
    "        return dec_tower\n",
    "\n",
    "    def init_sampler(self, mult):\n",
    "        enc_sampler = nn.ModuleList()\n",
    "        dec_sampler = nn.ModuleList()\n",
    "        for g in range(self.groups_per_scale):\n",
    "            num_c = int(self.num_channels_enc * mult)\n",
    "            cell = Conv2D(num_c, 2 * self.num_latent_per_group, kernel_size=3, padding=1, bias=True)\n",
    "            enc_sampler.append(cell)\n",
    "            if g != 0:\n",
    "                num_c = int(self.num_channels_dec * mult)\n",
    "                cell = nn.Sequential(\n",
    "                    nn.ELU(),\n",
    "                    Conv2D(num_c, 2 * self.num_latent_per_group, kernel_size=1, padding=0, bias=True))\n",
    "                dec_sampler.append(cell)\n",
    "        mult = mult/self.channel_mult\n",
    "        return enc_sampler, dec_sampler\n",
    "\n",
    "    def init_post_process(self, mult):\n",
    "        post_process = nn.ModuleList()\n",
    "        for b in range(self.num_postprocess_blocks):\n",
    "            for c in range(self.num_postprocess_cells):\n",
    "                if c == 0:\n",
    "                    arch = self.arch_instance['up_post']\n",
    "                    num_ci = int(self.num_channels_dec * mult)\n",
    "                    num_co = int(num_ci / self.channel_mult)\n",
    "                    cell = Cell(num_ci, num_co, cell_type='up_post', arch=arch, use_se=self.use_se)\n",
    "                    mult = mult / self.channel_mult\n",
    "                else:\n",
    "                    arch = self.arch_instance['normal_post']\n",
    "                    num_c = int(self.num_channels_dec * mult)\n",
    "                    cell = Cell(num_c, num_c, cell_type='normal_post', arch=arch, use_se=self.use_se)\n",
    "                post_process.append(cell)\n",
    "        self.mult = mult\n",
    "        return post_process\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.stem(2 * x - 1.0)\n",
    "        for cell in self.pre_process:\n",
    "            s = cell(s)\n",
    "        combiner_cells_enc = []\n",
    "        combiner_cells_s = []\n",
    "        all_z = []\n",
    "        for cell in self.enc_tower:\n",
    "            if cell.cell_type == 'combiner_enc':\n",
    "                combiner_cells_enc.append(cell)\n",
    "                combiner_cells_s.append(s)\n",
    "            else:\n",
    "                s = cell(s)\n",
    "        combiner_cells_enc.reverse()\n",
    "        combiner_cells_s.reverse()\n",
    "        idx_dec = 0\n",
    "        ftr = self.enc0(s)   #conv\n",
    "        param0 = self.enc_sampler[idx_dec](ftr) # another conv2d\n",
    "        mu_q, log_sig_q = torch.chunk(param0, 2, dim=1)\n",
    "        dist = Normal(mu_q, log_sig_q)\n",
    "        z, _ = dist.sample()   #z_0\n",
    "        all_z.append(z)\n",
    "        idx_dec = 0\n",
    "        s = self.prior_ftr0.unsqueeze(0) # random value\n",
    "        batch_size = z.size(0)\n",
    "        s = s.expand(batch_size, -1, -1, -1)\n",
    "        idx_dec = 0\n",
    "        for cell in self.dec_tower:\n",
    "            if cell.cell_type == 'combiner_dec':\n",
    "                if idx_dec > 0:\n",
    "                    ftr = combiner_cells_enc[idx_dec - 1](combiner_cells_s[idx_dec - 1], s)\n",
    "                    param = self.enc_sampler[idx_dec](ftr)\n",
    "                    mu_q, log_sig_q = torch.chunk(param, 2, dim=1)\n",
    "                    dist = Normal(mu_q, log_sig_q)\n",
    "                    z, _ = dist.sample()    # z_n\n",
    "                    all_z.append(z)\n",
    "                    #print(z.shape)\n",
    "                s = cell(s, z)\n",
    "                idx_dec += 1\n",
    "            else:\n",
    "                s = cell(s)\n",
    "\n",
    "        for cell in self.post_process:\n",
    "            s = cell(s)\n",
    "        # print(s.shape)\n",
    "        logits = self.image_conditional(s)\n",
    "        logits = self.projection(logits[...,-(self.input_size + self.hidden_size):])\n",
    "        return logits\n",
    "\n",
    "    def decoder_output(self, logits):\n",
    "        return NormalDecoder(logits)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
